# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13wq33pH6MxaK8QujEn7Tw-LRW7nOke9q
"""

from google.colab import files
uploaded = files.upload()

!pip install scikit-learn pandas matplotlib seaborn

import pandas as pd

# Load your uploaded dataset
df = pd.read_csv('online_shoppers_intention.csv')
print(df.shape)
df.head()

from sklearn.preprocessing import LabelEncoder

# Encode categorical columns
categorical_cols = ['Month', 'VisitorType', 'Weekend']
for col in categorical_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])

# Features & Target
X = df.drop(columns=['Revenue'])
y = df['Revenue']  # Target Variable

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

# Define base models
base_learners = [
    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),
    ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),
    ('lr', LogisticRegression(max_iter=1000))
]

# Meta model (stacking)
meta_model = LogisticRegression()

# Stacking ensemble
stacking_model = StackingClassifier(
    estimators=base_learners,
    final_estimator=meta_model,
    cv=5
)

# Train the stacking model
stacking_model.fit(X_train_scaled, y_train)

# Evaluate
y_pred_stack = stacking_model.predict(X_test_scaled)
print("Stacking Model Performance:")
print(classification_report(y_test, y_pred_stack))

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_scaled, y_train)
y_pred_rf = rf_model.predict(X_test_scaled)
print("Random Forest Alone Performance:")
print(classification_report(y_test, y_pred_rf))

import matplotlib.pyplot as plt
import seaborn as sns

importances = rf_model.feature_importances_
feature_names = X.columns

plt.figure(figsize=(12,8))
sns.barplot(x=importances, y=feature_names)
plt.title("Random Forest Feature Importances")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Collect Evaluation Metrics
models = ['Random Forest', 'Stacked Model']
accuracy = [
    accuracy_score(y_test, y_pred_rf),
    accuracy_score(y_test, y_pred_stack)
]
precision = [
    precision_score(y_test, y_pred_rf),
    precision_score(y_test, y_pred_stack)
]
recall = [
    recall_score(y_test, y_pred_rf),
    recall_score(y_test, y_pred_stack)
]
f1 = [
    f1_score(y_test, y_pred_rf),
    f1_score(y_test, y_pred_stack)
]

# Prepare Data for Plot
metrics = {
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1 Score': f1
}

# Plotting Comparison
plt.figure(figsize=(10, 6))
for metric_name, values in metrics.items():
    sns.barplot(x=models, y=values)
    plt.title(f'{metric_name} Comparison')
    plt.ylabel(metric_name)
    plt.ylim(0, 1)  # As these metrics are between 0 and 1
    for i, v in enumerate(values):
        plt.text(i, v + 0.02, f"{v:.2f}", ha='center', fontweight='bold')
    plt.show()

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

# Define hyperparameter grid for tuning
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5, 10],
    'max_features': ['sqrt', 'log2']
}

# Grid Search with 5-Fold Cross Validation (optimize F1 Score)
grid_search = GridSearchCV(RandomForestClassifier(random_state=42),
                           param_grid,
                           cv=5,
                           scoring='f1',
                           n_jobs=-1,
                           verbose=2)

# Fit the grid search model
grid_search.fit(X_train_scaled, y_train)

# Best parameters & score
print("Best Hyperparameters:", grid_search.best_params_)
print("Best F1 Score from Tuning:", grid_search.best_score_)

best_rf_model = grid_search.best_estimator_
y_pred_rf_tuned = best_rf_model.predict(X_test_scaled)

from sklearn.metrics import classification_report
print("Tuned Random Forest Performance:")
print(classification_report(y_test, y_pred_rf_tuned))

# Stacked model already trained earlier as `stacking_model`
y_pred_stack = stacking_model.predict(X_test_scaled)

print("Stacked Model Performance:")
print(classification_report(y_test, y_pred_stack))

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Collecting Metrics
models = ['Tuned Random Forest', 'Stacked Model']
accuracy = [
    accuracy_score(y_test, y_pred_rf_tuned),
    accuracy_score(y_test, y_pred_stack)
]
precision = [
    precision_score(y_test, y_pred_rf_tuned),
    precision_score(y_test, y_pred_stack)
]
recall = [
    recall_score(y_test, y_pred_rf_tuned),
    recall_score(y_test, y_pred_stack)
]
f1 = [
    f1_score(y_test, y_pred_rf_tuned),
    f1_score(y_test, y_pred_stack)
]

metrics = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1 Score': f1}

# Plotting Comparisons
plt.figure(figsize=(10, 6))
for metric_name, values in metrics.items():
    sns.barplot(x=models, y=values)
    plt.title(f'{metric_name} Comparison')
    plt.ylabel(metric_name)
    plt.ylim(0, 1)
    for i, v in enumerate(values):
        plt.text(i, v + 0.02, f"{v:.2f}", ha='center', fontweight='bold')
    plt.show()

from xgboost import XGBClassifier
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression

# Define base models
base_learners = [
    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),
    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42))
]

# Meta Learner
meta_model = LogisticRegression(max_iter=1000)

# Build Stacking Model
stack_model_xgb = StackingClassifier(
    estimators=base_learners,
    final_estimator=meta_model,
    cv=5
)

# Train
stack_model_xgb.fit(X_train_scaled, y_train)
y_pred_stack_xgb = stack_model_xgb.predict(X_test_scaled)

print("XGBoost Stacked Model Performance:")
print(classification_report(y_test, y_pred_stack_xgb))